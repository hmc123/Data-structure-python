#！usr/bin/env python
# -*- coding:utf-8 -*-
__author__ = 'HMC'

from lxml import etree
from multiprocessing.dummy import Pool
import requests, os, time, os, sys, mysql.connector

def conn_mysql():
    try:
        conn = mysql.connector.connect(user='root', password='HMC191030', database='douban', use_unicode=True)
    except:
        conn = mysql.connector.connect(user='root', password='HMC191030',  use_unicode=True)
    return conn

def insert_mysql(conn,a, b, c, d):
    cursor = conn.cursor()
    cursor.execute('create table if not exists doubancomment (id int primary key auto_increment, commentdate date,\
                   username varchar(20), userurl text, commentcontent text) ')
    cursor.execute('insert into doubancomment (commentdate,username,userurl,commentcontent) values (%s,%s,%s,%s)' % (a,b,c,d))
    cursor.close()


def savetext(result):
    path = 'c:\\users\\administrator\\desktop\\spider_save.txt'
    if os.path.isfile(path):
        with open(path, 'a', encoding='utf-8') as f:
            f.write('+'*30+'\r\n'+result['username']+'\r\n'+result['userurl']+'\r\n'+result['commentcontent']+'\r\n'+result['commentdate']+'\r\n')
    else:
        with open(path, 'w', encoding='utf-8') as f:
            f.write('+'*30+'\r\n'+result['username']+'\r\n'+result['userurl']+'\r\n'+result['commentcontent']+'\r\n'+result['commentdate']+'\r\n')


global urls  #用于保存第一次调用spider时的url，供获取下一页链接使用
urls = ''
def spider(url,flag=False):

    conn = conn_mysql() # link to mysql
    print('link to mysql_douban')

    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.118 UBrowser/5.2.3635.47 Safari/537.36'}
    global urls
    if not flag:
        urls = url

    html = requests.get(url, headers=headers)
    html.encoding = 'utf-8'
    convertxml = etree.HTML(html.text)
    extract = convertxml.xpath('//*[@class="comment"]')
    result = {}

    for each in extract:
        result['username'] = each.xpath('h3/span[2]/a/text()')[0].strip()
        result['userurl'] = each.xpath('h3/span[2]/a/@href')[0].strip()
        result['commentcontent'] = each.xpath('p/text()')[0].strip()
        try:
            result['commentdate'] = each.xpath('h3/span[2]/span[2]/text()')[0].strip()  #发帖时间在豆瓣位置是有变化的
        except:
            result['commentdate'] = each.xpath('h3/span[2]/span[1]/text()')[0].strip()
        savetext(result)
        print('dd')
        #insert_mysql(conn, result['commentdate'], result['username'], result['userurl'], result['commentcontent']) # insert records into mysql
    conn.close()
    nextpage = convertxml.xpath('//*[@class="next"]/@href')
    if nextpage:
        print(urls + nextpage[0])
        spider(urls + nextpage[0], True)
    else:
        print('no.....')


if __name__ == '__main__':
    #spider('http://movie.douban.com/subject/3338862/comments')
    st = time.time()
    pool = Pool(2)
    pool.map(spider, ['http://movie.douban.com/subject/3338862/comments'])
    pool.close()
    pool.join()
    usedtime = time.time() - st
    print('used time:%2fs'% usedtime)
